{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Essential libraries**"
      ],
      "metadata": {
        "id": "51aev66FQUUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n",
        "pip install torch\n",
        "pip install pandas\n",
        "pip install scipy"
      ],
      "metadata": {
        "id": "QBdKzIZYQbyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data loading and preprocessing**\n",
        "\n",
        "*   optional, if not done before\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n-Yf6FJ6QnHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()                 # Lowercase\n",
        "    text = re.sub(r'http\\S+', '', text)     # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)        # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)        # Remove hashtags\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)    # Remove punctuation/numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Load your data (assuming you have a CSV)\n",
        "# You would have already filtered this for tariff/industry keywords\n",
        "df = pd.read_csv(\"your_filtered_text_data.csv\")\n",
        "\n",
        "# Apply the preprocessing\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"Preprocessing complete.\")\n",
        "print(df[['text', 'processed_text']].head())"
      ],
      "metadata": {
        "id": "nHA093LmQ3ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Pre-trained ProsusAI/FinBERT Model**"
      ],
      "metadata": {
        "id": "NqEu4w55RSiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_NAME = \"ProsusAI/finbert\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"Loaded model: {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "RezRPODRRgVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the Sentiment Analysis Function**"
      ],
      "metadata": {
        "id": "fgq01tazRqex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from scipy.special import softmax\n",
        "\n",
        "def get_finbert_sentiment(text_list):\n",
        "    \"\"\"\n",
        "    Analyzes a list of texts and returns sentiment details.\n",
        "    \"\"\"\n",
        "\n",
        "    # We run this in \"no_grad\" mode, as we are not training the model\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # 1. Tokenize the text\n",
        "        inputs = tokenizer(text_list,\n",
        "                           padding=True,\n",
        "                           truncation=True,\n",
        "                           max_length=128,         # change len if needed\n",
        "                           return_tensors=\"pt\")\n",
        "\n",
        "        # 2. Get model outputs (logits)\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # 3. Convert logits to probabilities (0-1) using softmax\n",
        "        scores = softmax(outputs.logits.numpy(), axis=1)\n",
        "\n",
        "    # 4. Map probabilities to labels\n",
        "    labels = model.config.id2label\n",
        "    results = []\n",
        "\n",
        "    for score in scores:\n",
        "        # Get the scores for each class\n",
        "        prob_pos = score[labels.index('positive')]\n",
        "        prob_neg = score[labels.index('negative')]\n",
        "        prob_neu = score[labels.index('neutral')]\n",
        "\n",
        "        # Get the final predicted label\n",
        "        pred_label = labels[score.argmax()]\n",
        "\n",
        "        # 5. Calculate the continuous score as per your plan\n",
        "        # Formula: (+1 * P(pos)) - (1 * P(neg))\n",
        "        sentiment_score = (1 * prob_pos) - (1 * prob_neg)\n",
        "\n",
        "        results.append({\n",
        "            'label': pred_label,\n",
        "            'sentiment_score': sentiment_score,\n",
        "            'positive_prob': prob_pos,\n",
        "            'negative_prob': prob_neg,\n",
        "            'neutral_prob': prob_neu\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "nEqNplMHRtut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Analysis and Aggregate to Daily Index**"
      ],
      "metadata": {
        "id": "rUA1HmmWR9SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get a list of all texts to analyze\n",
        "# (Running in batches is much faster than one by one)\n",
        "texts_to_analyze = df['processed_text'].tolist()\n",
        "\n",
        "# 2. Run the sentiment analysis\n",
        "print(\"Running FinBERT analysis... This may take a while.\")\n",
        "sentiment_results = get_finbert_sentiment(texts_to_analyze)\n",
        "print(\"Analysis complete.\")\n",
        "\n",
        "# 3. Add results back to your DataFrame\n",
        "sentiment_df = pd.DataFrame(sentiment_results)\n",
        "df = pd.concat([df.reset_index(drop=True), sentiment_df], axis=1)\n",
        "\n",
        "# 4. Create the final Daily Sentiment Index\n",
        "# Ensure your date column is in datetime format\n",
        "df['doc_date'] = pd.to_datetime(df['doc_date'])\n",
        "\n",
        "# Aggregate by industry and date, calculating the mean score\n",
        "daily_sentiment_index = df.groupby(\n",
        "    ['industry', df['doc_date'].dt.date]\n",
        ")['sentiment_score'].mean().reset_index()\n",
        "\n",
        "# Rename for clarity\n",
        "daily_sentiment_index = daily_sentiment_index.rename(\n",
        "    columns={'doc_date': 'date', 'sentiment_score': 'S_i_t'}\n",
        ")\n",
        "\n",
        "# 5. Save your final index\n",
        "daily_sentiment_index.to_csv(\"daily_industry_sentiment_index.csv\", index=False)\n",
        "\n",
        "print(\"\\n--- Daily Industry Sentiment Index (S_i,t) ---\")\n",
        "print(daily_sentiment_index.head())"
      ],
      "metadata": {
        "id": "k9qTbXrASBDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This outputs a CSV file (daily_industry_sentiment_index.csv) with three columns: industry, date, and S_i_t.\n",
        "\n",
        "This S_i_t (sentiment index) is the exact variable  needed to proceed with\n",
        "\n",
        "\n",
        "Plotting :  \"Daily sentiment index... with vertical lines for tariff dates\".\n",
        "\n"
      ],
      "metadata": {
        "id": "vojKtm5bSW1O"
      }
    }
  ]
}